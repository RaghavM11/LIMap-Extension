{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dcolli23/code/LIMap-Extension\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "REPO_DIR = Path(\".\").resolve().parents[1]\n",
    "# print(REPO_DIR)\n",
    "sys.path.append(REPO_DIR.as_posix())\n",
    "\n",
    "from limap_extension.optical_flow import motion_segmentation, Args, OpticalFlow\n",
    "from limap_extension.img_cloud_transforms import reproject_img, uvz_ned_to_xyz_cam, xyz_cam_to_uvz_ned, index_img_with_uv_coords, get_uv_coords, imgs_to_clouds_np\n",
    "from limap_extension.transforms_spatial import get_transform_matrix_from_pose_array\n",
    "from limap_extension.bounding_box import BoundingBox\n",
    "\n",
    "# from limap_extension.visualization.rerun.figure_factory import FigureFactory\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIAL_DIR = REPO_DIR / \"datasets\" / \"ocean\" / \"Hard\" / \"P006\"\n",
    "TRIAL_DIR = REPO_DIR / \"datasets\" / \"carwelding\" / \"easy\" / \"P007\"\n",
    "# FRAME_1 = 550\n",
    "# FRAME_2 = 551\n",
    "FRAME_1 = 127\n",
    "FRAME_2 = 128\n",
    "RAFT_PATH = REPO_DIR / \"raft-sintel.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data.\n",
    "frame_str = f\"{FRAME_1:06d}\"\n",
    "rgb_1 = np.array(Image.open(TRIAL_DIR / \"image_left\" / f\"{frame_str}_left.png\"))\n",
    "depth_1 = np.load(TRIAL_DIR / \"depth_left\" / f\"{frame_str}_left_depth.npy\")\n",
    "\n",
    "frame_str = f\"{FRAME_2:06d}\"\n",
    "rgb_2 = np.array(Image.open(TRIAL_DIR / \"image_left\" / f\"{frame_str}_left.png\"))\n",
    "depth_2 = np.load(TRIAL_DIR / \"depth_left\" / f\"{frame_str}_left_depth.npy\")\n",
    "\n",
    "cam_poses = np.loadtxt(TRIAL_DIR / \"pose_left.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opting to crop the depth images before input to reprojection\n",
    "depth_max = 50\n",
    "depth_1 = np.clip(depth_1, None, depth_max)\n",
    "depth_2 = np.clip(depth_2, None, depth_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img_pair(rgb, depth, img_slice: BoundingBox = None):\n",
    "    if img_slice is not None:\n",
    "        rgb = img_slice.crop_img(rgb)\n",
    "        depth = img_slice.crop_img(depth)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(rgb)\n",
    "\n",
    "    # depth = np.clip(depth, 0, 10)\n",
    "    im1 = ax[1].imshow(depth)\n",
    "    fig.colorbar(im1, ax=ax[1])\n",
    "\n",
    "\n",
    "# crop_box = BoundingBox(200, 350, 125, 275)\n",
    "# crop_box = BoundingBox(0, 100, 350, -1)\n",
    "# crop_box = BoundingBox(325, 450, 180, 220)\n",
    "crop_box = BoundingBox(0, -1, 0, -1)\n",
    "display_img_pair(rgb_1, depth_1, crop_box)\n",
    "display_img_pair(rgb_2, depth_2, crop_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = OpticalFlow(None)\n",
    "flow.load_model(RAFT_PATH, Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp1 = cam_poses[FRAME_1, :]\n",
    "cp2 = cam_poses[FRAME_2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_1 = get_transform_matrix_from_pose_array(cp1)\n",
    "pose_2 = get_transform_matrix_from_pose_array(cp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud_1, _ = imgs_to_clouds_np(rgb_1, depth_1, CAM_INTRINSIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject the image at time t to the image frame at time t+1\n",
    "img_1_in_frame_2, depth_1_in_frame_2, mask_valid_projection, valid_bbox = reproject_img(\n",
    "    rgb_1, depth_1, pose_1, pose_2)\n",
    "\n",
    "display_img_pair(img_1_in_frame_2, depth_1_in_frame_2)\n",
    "\n",
    "# valid_bbox = BoundingBox(0, -1, 0, -1)\n",
    "\n",
    "print(valid_bbox)\n",
    "\n",
    "rgb_1_cropped = valid_bbox.crop_img(img_1_in_frame_2)\n",
    "depth_1_cropped = valid_bbox.crop_img(depth_1_in_frame_2)\n",
    "rgb_2_cropped = valid_bbox.crop_img(rgb_2)\n",
    "depth_2_cropped = valid_bbox.crop_img(depth_2)\n",
    "mask_valid_projection_cropped = valid_bbox.crop_img(mask_valid_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img_pair(valid_bbox.crop_img(rgb_1), valid_bbox.crop_img(depth_1), crop_box)\n",
    "display_img_pair(rgb_1_cropped, depth_1_cropped, crop_box)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2)\n",
    "# ax[0].imshow(rgb_2_cropped)\n",
    "# ax[1].imshow(depth_2_cropped)\n",
    "display_img_pair(rgb_2_cropped, depth_2_cropped, crop_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_disparity = np.linalg.norm(rgb_1_cropped.astype(float) - rgb_2_cropped.astype(float), axis=-1)\n",
    "\n",
    "# mean_disparity = np.mean(rgb_disparity)\n",
    "# thresh_min = mean_disparity + np.std(rgb_disparity)\n",
    "thresh_min = 0\n",
    "rgb_disparity = np.clip(rgb_disparity, thresh_min, None)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(rgb_disparity)\n",
    "# plt.title(\"RGB Disparity\")\n",
    "\n",
    "depth_disparity = np.abs(depth_1_cropped - depth_2_cropped)\n",
    "print(\"Min depth disparity:\", np.min(depth_disparity))\n",
    "# mean_disparity = np.mean(depth_disparity)\n",
    "# thresh_min = mean_disparity + np.std(depth_disparity)\n",
    "# depth_disparity = np.clip(depth_disparity, thresh_min, None)\n",
    "# depth_disparity = np.clip(depth_disparity, 0, 0.2)\n",
    "# plt.figure()\n",
    "# plt.imshow(depth_disparity)\n",
    "# plt.title(\"Depth Disparity\")\n",
    "# plt.colorbar()\n",
    "\n",
    "depth_disparity = np.clip(depth_disparity, 0, 0.5)\n",
    "display_img_pair(rgb_disparity, depth_disparity, crop_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_box.crop_img(depth_disparity).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_1 = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "# test_1[40:60, 40:60, :] = 255\n",
    "# test_2 = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "# test_2[40:60, 50:70, :] = 255\n",
    "# _, flow_test = flow.infer_flow(test_1, test_2)\n",
    "# flow_test = flow_up = flow_test[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(test_1)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(test_2)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(flow_test[..., 0])\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(flow_test[..., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_low, flow_up = flow.infer_flow(rgb_1_cropped, rgb_2_cropped)\n",
    "# flow_low, flow_up = flow.infer_flow(rgb_2_cropped, rgb_1_cropped)\n",
    "# flow_low, flow_up = flow.infer_flow(rgb_1, rgb_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cropped shape:\", rgb_1_cropped.shape)\n",
    "print(\"Flow up shape:\", flow_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(flow_up, flow_low)\n",
    "flow_up = flow_up[0].permute(1, 2, 0).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_up.shape\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(crop_box.crop_img(flow_up[..., 0]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Flow u\")\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(crop_box.crop_img(flow_up[..., 1]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Flow v\")\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.linalg.norm(flow_up, axis=-1))\n",
    "plt.colorbar()\n",
    "plt.title(\"Flow Magnitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"mag shape:\", mag.shape)\n",
    "print(\"rgb 1 cropped shape:\", rgb_1_cropped.shape)\n",
    "\n",
    "# 4. Use the flow to, for each pixel, compute the index at time t + 1 from time t (just each pixel's\n",
    "#    image space coordinate in image 1's transformed frame)\n",
    "#     1. This would entail computing the dx, dy image space coordinate change from the flow's\n",
    "#        magnitude and angle channels (output), then adding that to the meshgrid coordinates (the\n",
    "#        coordinates at time t)\n",
    "img_width = rgb_1_cropped.shape[1]\n",
    "img_height = rgb_1_cropped.shape[0]\n",
    "# v_coords_1, u_coords_1 = np.meshgrid(np.arange(img_width), np.arange(img_height))\n",
    "u_coords_1, v_coords_1 = get_uv_coords(img_height, img_width)\n",
    "u_coords_1 = u_coords_1.flatten()\n",
    "v_coords_1 = v_coords_1.flatten()\n",
    "z_vals_1 = depth_1_cropped.flatten()\n",
    "\n",
    "# mag_flat = mag.flatten()\n",
    "\n",
    "# TODO: Is this angle in the same coordinate frame as the image?\n",
    "# e.g. might have to flip the angle, rotate by 90 degrees, etc.\n",
    "# angle_flat = angle.flatten() / 180 * np.pi\n",
    "\n",
    "# du = np.round(mag_flat * np.cos(angle_flat)).astype(int)\n",
    "# dv = np.round(mag_flat * np.sin(angle_flat)).astype(int)\n",
    "dv = np.round(flow_up[..., 0].flatten()).astype(int)\n",
    "du = np.round(flow_up[..., 1].flatten()).astype(int)\n",
    "\n",
    "# Subtracting coordinates since coordinate frames for flow and images are different\n",
    "u_coords_2 = u_coords_1 + du\n",
    "v_coords_2 = v_coords_1 + dv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_coords_1.max()\n",
    "print(rgb_1_cropped[v_coords_1, u_coords_1, 0].shape)\n",
    "print(img_height * img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cut out all coordinates that are out of bounds\n",
    "# mask_valid_points = np.ones((img_height, img_width), dtype=bool)\n",
    "coords_valid = np.ones_like(u_coords_2, dtype=bool)\n",
    "# us_invalid = (u_coords_2 < 0) & (u_coords_2 >= img_width)\n",
    "# vs_invalid = (v_coords_2 < 0) & (v_coords_2 >= img_height)\n",
    "coords_valid[u_coords_2 < 0] = False\n",
    "coords_valid[u_coords_2 >= img_width] = False\n",
    "coords_valid[v_coords_2 < 0] = False\n",
    "coords_valid[v_coords_2 >= img_height] = False\n",
    "# coords_invalid = us_invalid | vs_invalid\n",
    "# mask_valid_points[coords_invalid] = False\n",
    "proj_valid = mask_valid_projection_cropped.flatten()\n",
    "coords_valid = coords_valid & proj_valid\n",
    "\n",
    "u_coords_2_valid = u_coords_2[coords_valid]\n",
    "v_coords_2_valid = v_coords_2[coords_valid]\n",
    "\n",
    "z_vals_2 = depth_2_cropped[v_coords_2_valid, u_coords_2_valid]\n",
    "# z_vals_2 = depth_2_cropped[v_coords_2_valid, u_coords_2_valid]\n",
    "# z_coords_2 = depth_2_cropped.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_shape = rgb_1_cropped.shape\n",
    "\n",
    "print(crop_shape)\n",
    "\n",
    "print(\"Image 1 shape:\", rgb_1.shape)\n",
    "print(\"u min crop:\", valid_bbox.u_min)\n",
    "print(\"u max crop:\", valid_bbox.u_max)\n",
    "print(\"v min crop:\", valid_bbox.v_min)\n",
    "print(\"v max crop:\", valid_bbox.v_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_coords_1_valid = u_coords_1[coords_valid]\n",
    "v_coords_1_valid = v_coords_1[coords_valid]\n",
    "z_vals_1_valid = z_vals_1[coords_valid]\n",
    "\n",
    "# TODO: The intrinsic expects the UV coordinates to be generated based on images that are (480, 640)\n",
    "# but due to how we're cropping things, the UV coordinates are generated based on the cropped images\n",
    "# that could be quite different. We need to adjust the UV coordinates to be based on the original\n",
    "# image size.\n",
    "us_fixed = u_coords_1_valid + valid_bbox.u_min\n",
    "vs_fixed = v_coords_1_valid + valid_bbox.v_min\n",
    "\n",
    "xyz_1 = uvz_ned_to_xyz_cam(u_coords_1_valid, v_coords_1_valid, z_vals_1_valid)\n",
    "xyz_2 = uvz_ned_to_xyz_cam(u_coords_2_valid, v_coords_2_valid, z_vals_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_disparity = z_vals_1_valid - z_vals_2\n",
    "\n",
    "print(\"Min disparity:\", z_disparity.min())\n",
    "print(\"Max disparity:\", z_disparity.max())\n",
    "\n",
    "z_disparity[z_disparity < -2] = -2\n",
    "z_disparity[z_disparity > 2] = 2\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(z_disparity[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Project these two images into a point cloud\n",
    "# convert the point cloud using grid coordinates and depths\n",
    "# Just use intrinsic\n",
    "# xyz_1 = np.random.rand(IMG_HEIGHT * IMG_WIDTH, 3)\n",
    "# xyz_2 = np.random.rand(IMG_HEIGHT * IMG_WIDTH, 3)\n",
    "\n",
    "# 6. Index point cloud 1 with coordinate at time t, point cloud 2 with coordinate at time t +1,\n",
    "# 7. Compute Euclidean distance\n",
    "flow_3d = np.linalg.norm(xyz_1 - xyz_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flow min:\", flow_3d.min())\n",
    "print(\"Flow max:\", flow_3d.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_dynamic_mask = flow_3d > 0.5  # meters\n",
    "# coords_dynamic_1 = xyz_1[points_dynamic_mask, :]\n",
    "coords_dynamic_2 = xyz_2[points_dynamic_mask, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. reproject the points back into image space with the distance being the value at each pixel\n",
    "mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "# reproject to image coordinates using funciton and intrinsic\n",
    "us, vs, zs = xyz_cam_to_uvz_ned(coords_dynamic_2, is_rounding_to_int=True)\n",
    "\n",
    "from limap_extension.img_cloud_transforms import find_valid_uv_coords\n",
    "\n",
    "where_valid = find_valid_uv_coords(us, vs, img_height, img_width)\n",
    "us = us[where_valid]\n",
    "vs = vs[where_valid]\n",
    "\n",
    "mask[vs, us] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img_pair(rgb_1_cropped, rgb_2_cropped)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_reconstructed = np.zeros((img_height, img_width), dtype=np.float32)\n",
    "depth_reconstructed[vs, us] = zs\n",
    "\n",
    "threshold = 30\n",
    "depth_reconstructed[depth_reconstructed > threshold] = threshold\n",
    "plt.figure()\n",
    "plt.imshow(depth_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "limap_extension",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
